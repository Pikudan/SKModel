\documentclass[14pt,a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage[russian]{babel}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{float}
\usepackage{geometry}
\usepackage{booktabs}
\usepackage{longtable}
\usepackage{multirow}
\usepackage{makecell}
\usepackage{pgfplots}
\pgfplotsset{compat=1.18}

\geometry{left=2cm,right=2cm,top=2cm,bottom=2cm}
\begin{document}

\begin{titlepage}
    \begin{figure}
        \centering
        
        \includegraphics[scale=0.7]{logo.eps}
        \label{fig:my_label}
    \end{figure}
\vspace{4cm}
\centering
\vspace{\stretch{1}}
\begin{center}
    
Отчет по курсу \\ 
«Суперкомпьютерное моделирование и технологии» \\

Студент: Пикуров Даниил \\ Группа: № 617
\vspace{2cm}

Задача Дирихле для уравнения Пуассона в криволинейной области \\
Задание 5: Трапеция с вершинами $A(-3,0), B(3,0), C(2,3), D(-2,3)$

\end{center}
\vspace{15cm}
\begin{center}
    Москва, 2025
\end{center}
\end{titlepage}


\newpage
\section{Математическая постановка задачи}

Требуется решить двумерную задачу Дирихле для уравнения Пуассона в криволинейной области (трапеции):

\begin{equation}
-\Delta u = f(x, y), \quad (x, y) \in D
\end{equation}

\begin{equation}
u(x, y) = 0, \quad (x, y) \in \gamma
\end{equation}

где:
\begin{itemize}
\item $D$ - трапеция с вершинами $A(-3,0), B(3,0), C(2,3), D(-2,3)$
\item $\gamma$ - граница области $D$
\item $f(x, y) = 1$ - правая часть уравнения
\end{itemize}

\section{Численный метод решения}

\subsection{Метод фиктивных областей}

Для решения задачи в криволинейной области применяется метод фиктивных областей. Исходная область $D$ вкладывается в прямоугольник $\Pi = \{(x,y) : -3 \le x \le 3, 0 \le y \le 3\}$.

Вводится фиктивная область $\hat{D} = \Pi \setminus \overline{D}$ и решается задача:

\begin{equation}
-\frac{\partial}{\partial x} \left( k(x,y) \frac{\partial v}{\partial x} \right) - \frac{\partial}{\partial y} \left( k(x,y) \frac{\partial v}{\partial y} \right) = F(x,y)
\end{equation}

\begin{equation}
v(x,y) = 0, \quad (x,y) \in \Gamma
\end{equation}

где:
\begin{itemize}
\item $k(x,y) = \begin{cases} 
1, & (x,y) \in D \\
1/\varepsilon, & (x,y) \in \hat{D}
\end{cases}$
\item $F(x,y) = \begin{cases} 
f(x,y), & (x,y) \in D \\
0, & (x,y) \in \hat{D}
\end{cases}$
\item $\varepsilon = h^2$, $h = \max(h_x, h_y)$
\end{itemize}

\subsection{Разностная схема}

На прямоугольной сетке $\bar{\omega}_h = \bar{\omega}_1 \times \bar{\omega}_2$ строится разностная схема:

\begin{equation}
-\frac{1}{h_1} \left( a_{i+1,j} \frac{w_{i+1,j} - w_{ij}}{h_1} - a_{ij} \frac{w_{ij} - w_{i-1,j}}{h_1} \right) - \frac{1}{h_2} \left( b_{ij+1} \frac{w_{ij+1} - w_{ij}}{h_2} - b_{ij} \frac{w_{ij} - w_{ij-1}}{h_2} \right) = F_{ij}
\end{equation}

Коэффициенты вычисляются по формулам:
\begin{equation}
a_{ij} = \frac{1}{h_2} \int_{y_{j-1/2}}^{y_{j+1/2}} k(x_{i-1/2}, t) dt, \quad b_{ij} = \frac{1}{h_1} \int_{x_{i-1/2}}^{x_{i+1/2}} k(t, y_{j-1/2}) dt
\end{equation}

\newpage
\subsection{Метод сопряженных градиентов}

\hspace{0.5cm}
Для решения СЛАУ применяется метод сопряженных градиентов с диагональным предобуславливанием. Алгоритм:

\begin{enumerate}
\item Начальное приближение: $w^{(0)} = 0$
\item Невязка: $r^{(0)} = B - Aw^{(0)}$
\item Решение системы предобуславливания: $Dz^{(0)} = r^{(0)}$
\item Направление спуска: $p^{(1)} = z^{(0)}$
\item Итерационный процесс:
    

\begin{align*}
\alpha_k &= \frac{(z^{(k-1)}, r^{(k-1)})}{(Ap^{(k)}, p^{(k)})} \\
w^{(k)} &= w^{(k-1)} + \alpha_k p^{(k)} \\
r^{(k)} &= r^{(k-1)} - \alpha_k Ap^{(k)} \\
\end{align*}
\begin{align*}
Dz^{(k)} &= r^{(k)} \\
\beta_k &= \frac{(z^{(k)}, r^{(k)})}{(z^{(k-1)}, r^{(k-1)})} \\
p^{(k+1)} &= z^{(k)} + \beta_k p^{(k)}
\end{align*}
\end{enumerate}

\section{Параллельная реализация}

\subsection{OpenMP реализация}

\hspace{0.5cm}
Для параллелизации последовательного кода использованы директивы OpenMP:

\begin{itemize}
\item \texttt{\#pragma omp parallel for collapse(2)} - для вложенных циклов по сетке
\item \texttt{\#pragma omp parallel for reduction(+:result)} - для скалярных произведений
\item \texttt{omp\_set\_num\_threads(num\_threads)} - установка числа потоков
\item \texttt{omp\_get\_wtime()} - для измерения времени
\end{itemize}

\vspace{0.5cm}
Основные параллелизуемые участки:
\begin{itemize}
\item Инициализация сеток и масок
\item Вычисление коэффициентов $a_{ij}$, $b_{ij}$, $F_{ij}$
\item Применение оператора $A$
\item Решение системы с предобуславливателем $D$
\item Скалярные произведения и обновления векторов
\end{itemize}

\newpage
\subsection{MPI реализация}

\hspace{0.5cm}
Для распределенной параллелизации использована библиотека MPI. Реализован алгоритм двумерного разбиения прямоугольной области $\Pi$ на домены (подобласти) с соблюдением следующих условий:

\begin{enumerate}
\item Отношение количества узлов по переменным $x$ и $y$ в каждом домене принадлежит диапазону $[1/2, 2]$
\item Количество узлов по переменным $x$ и $y$ любых двух доменов отличается не более, чем на единицу
\end{enumerate}


\vspace{0.5cm}
Основные компоненты MPI-реализации:
\begin{itemize}
\item \texttt{MPI\_Init}, \texttt{MPI\_Finalize} - инициализация и завершение работы с MPI
\item \texttt{MPI\_Comm\_rank}, \texttt{MPI\_Comm\_size} - определение ранга и размера коммуникатора
\item \texttt{MPI\_Sendrecv} - обмен граничными данными между соседними процессами в четырех направлениях (верх, низ, лево, право)
\item \texttt{MPI\_Allreduce} - глобальная редукция для скалярных произведений и вычисления нормы невязки (обеспечивает синхронизацию процессов)
\item \texttt{MPI\_Reduce} - сбор максимального времени выполнения среди всех процессов
\item \texttt{MPI\_Gatherv} - сбор локальных решений на процесс 0
\item \texttt{MPI\_Wtime} - измерение времени выполнения
\end{itemize}

\vspace{0.5cm}
Особенности реализации:
\begin{itemize}
\item Двумерная декомпозиция области с использованием структуры \texttt{ProcessTopology2D}, хранящей информацию о размерах локального домена, координатах процесса в сетке и границах с ghost cells
\item Каждый процесс хранит локальную часть сетки с граничными ячейками (ghost cells) для обмена данными с соседними процессами
\item Использование структуры \texttt{ExchangeBuffers} для предвыделенных буферов обмена граничными данными
\item Скалярные произведения и нормы вычисляются локально на каждом процессе, затем суммируются через \texttt{MPI\_Allreduce} для получения глобального значения. 
\item Финальное решение собирается на процессе 0 с использованием \texttt{MPI\_Gatherv} для сохранения результатов
\end{itemize}



\subsection{Гибридная MPI+OpenMP реализация}

\hspace{0.5cm}
Для повышения производительности разработана гибридная реализация, объединяющая преимущества MPI и OpenMP. В MPI-код добавлены директивы OpenMP для параллелизации вычислений внутри каждого процесса.

\vspace{0.5cm}
Основные особенности гибридной реализации:
\begin{itemize}
\item Двумерное разбиение области между процессами MPI (как в чистой MPI реализации)
\item Параллелизация вычислений внутри каждого процесса с помощью OpenMP
\item Использование директив \texttt{\#pragma omp parallel for collapse(2)} для вложенных циклов по сетке
\item Использование \texttt{\#pragma omp parallel for collapse(2) reduction(+:local\_result)} для скалярных произведений и подсчета неизвестных
\item Параллелизация всех основных операций: инициализация сеток, вычисление коэффициентов $a_{ij}$, $b_{ij}$, $F_{ij}$, применение оператора $A$, решение системы с предобуславливателем $D$, обновления векторов в методе сопряженных градиентов
\item Параллелизация обмена граничными данными (копирование в/из предвыделенных буферов)
\item Каждый процесс может использовать несколько потоков OpenMP для обработки своего локального домена
\end{itemize}

\subsection{MPI+CUDA реализация}

\hspace{0.5cm}
Для ускорения вычислений разработана реализация с использованием MPI и CUDA, которая переносит основные вычисления на графические процессоры (GPU).

\vspace{0.5cm}
Основные особенности MPI+CUDA реализации:
\begin{itemize}
\item Двумерное разбиение области между процессами MPI (как в чистой MPI реализации)
\item Основные вычисления выполняются на GPU с помощью CUDA ядер:
\begin{itemize}
\item \texttt{apply\_A\_kernel} - применение оператора $A$ к вектору
\item \texttt{solve\_D\_kernel} - решение системы с предобуславливателем $D$
\item \texttt{dot\_product\_kernel} - вычисление скалярных произведений
\item \texttt{update\_vectors\_kernel} - обновление векторов в методе сопряженных градиентов
\item \texttt{update\_p\_kernel} - обновление направления спуска
\item \texttt{init\_residual\_kernel} - инициализация невязки
\end{itemize}
\item Редукция выполняется с помощью библиотеки Thrust на GPU (без использования разделяемой памяти и atomic операций)
\item Каждый MPI процесс использует свой GPU (распределение через \texttt{rank \% device\_count})
\item Данные копируются между CPU и GPU только при необходимости (обмен границами, сбор результатов)
\item Измеряется детальное время выполнения всех операций: копирование данных на GPU, вычисления на GPU, коммуникации MPI, сбор результатов
\end{itemize}

\vspace{0.5cm}
Технические детали реализации:
\begin{itemize}
\item Использование \texttt{cudaMalloc} для выделения памяти на GPU
\item \texttt{cudaMemcpy} для копирования данных между CPU и GPU
\item CUDA события (\texttt{cudaEvent\_t}) для точного измерения времени выполнения операций на GPU
\item Ограничение compute capability до 3.5 (sm\_35) или 6.0 (sm\_60) в соответствии с требованиями задания
\item Компиляция через Makefile с переменными \texttt{ARCH} и \texttt{HOST\_COMP}
\end{itemize}


\newpage
\section{Результаты расчетов}


\subsection{Последовательная программа}

\begin{table}[H]
\centering
\caption{Результаты последовательных расчетов (малые сетки)}
\begin{tabular}{cccccc}
\toprule
Сетка $(M \times N)$ & Число итераций & Время (с) \\
\midrule
$10 \times 10$ & 33 & 0.00022 \\
$20 \times 20$ & 61 & 0.00116 \\
$40 \times 40$ &  119 & 0.00726 \\
\bottomrule
\end{tabular}
\end{table}

\subsection{OpenMP программа для малых сеток}
\begin{table}[H]
\centering
\caption{Результаты OpenMP расчетов (сетка $40 \times 40$)}
\begin{tabular}{ccccc}
\toprule
Число потоков & Число итераций & Время (с) & Ускорение \\
\midrule
1  & 119 & 0.00961 & 0.76 \\
4 & 119 & 0.00679 & 1.07  \\
16 & 119 & 0.01708 & 0.43 \\
\bottomrule
\end{tabular}
\end{table}
\subsection{OpenMP программа для больших сеток}

\begin{table}[H]
\centering
\caption{Таблица с результатами расчетов на ПВС IBM Polus (OpenMP код)}
\begin{tabular}{ccccc}
\toprule
\makecell{Количество \\ OpenMP-нитей} & \makecell{Число точек \\ сетки} & \makecell{Число \\ итераций} & \makecell{Время решения \\(с)} & Ускорение \\
\midrule

2 & $400 \times 600$ & 1782 & 9.849 & 1.00 \\
4 & $400 \times 600$ & 1782 & 5.912 & 1.67 \\
8 & $400 \times 600$ & 1782 & 3.849 & 2.56 \\
16 & $400 \times 600$ & 1782 & 3.165 & 3.11 \\

4 & $800 \times 1200$ & 3596 & 43.907 & 1.00  \\
8 & $800 \times 1200$ & 3596 & 28.979 & 1.52  \\
16 & $800 \times 1200$ & 3596 & 23.4077 & 1.88 \\
32 & $800 \times 1200$ & 3596 & 27.563 & 1.59 \\
\bottomrule
\end{tabular}
\end{table}

\newpage
\subsection{MPI программа}

\begin{table}[H]
\centering
\caption{Результаты MPI расчетов (сетка $40 \times 40$)}
\begin{tabular}{ccccc}
\toprule
\makecell{Количество \\ процессов MPI} & \makecell{Число точек \\ сетки} & \makecell{Число \\ итераций} & \makecell{Время \\(с)} & Ускорение \\
\midrule
1 & 119 & 0.00680205 & 1.07 \\
2 & 119 & 0.0043437 & 1.67 \\
4 & 119 & 0.00482718 & 1.50 \\
\bottomrule
\end{tabular}
\end{table}

\begin{table}[H]
\centering
\caption{Таблица 2: Результаты расчетов MPI-программы на ПВС IBM Polus}
\begin{tabular}{cccccc}
\toprule
\makecell{Количество \\ процессов MPI} & \makecell{Число точек \\ сетки} & \makecell{Число \\ итераций} & \makecell{Время решения \\(с)} & Ускорение \\
\midrule
2 & $400 \times 600$ & 1782 & 6.50059 & 1.00 \\
4 & $400 \times 600$ & 1782 & 3.52396 & 1.85 \\
8 & $400 \times 600$ & 1782 & 1.8521 & 3.51 \\
16 & $400 \times 600$ & 1782 & 1.30841 & 4.97 \\

4 & $800 \times 1200$ & 3596 & 26.9512 & 1.00  \\
8 & $800 \times 1200$ & 3596 & 14.113 & 1.91  \\
16 & $800 \times 1200$ & 3596 & 9.07348 & 2.97 \\
20 & $800 \times 1200$ & 3596 & 8.33331 & 3.23 \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Гибридная MPI+OpenMP реализация}

\vspace{0.5cm}


\begin{table}[H]
\centering
\caption{Таблица 3: Результаты расчетов гибридной MPI+OpenMP программы (сетка $40 \times 40$)}
\begin{tabular}{ccccc}
\toprule
\makecell{Количество \\ процессов} & \makecell{Потоков \\ на процесс} & \makecell{Число \\ итераций} & \makecell{Время \\(с)} & Ускорение \\
\midrule
1 & 4 & 119 & 0.00748 & 0.97 \\
2 & 4 & 119 & 0.00611 & 1.19 \\
\bottomrule
\end{tabular}
\end{table}

\begin{table}[H]
\centering
\caption{Таблица 3: Результаты расчетов гибридной MPI+OpenMP программы на ПВС IBM Polus}
\begin{tabular}{cccccc}
\toprule
\makecell{Количество \\ процессов \\ MPI} & \makecell{Количество \\ OpenMP-нитей \\ в процессе} & \makecell{Число точек \\ сетки} & \makecell{Число \\ итераций} & \makecell{Время решения \\(с)} & Ускорение \\
\midrule
2 & 1 & $400 \times 600$ & 1782 & 9.40471 & 1.00 \\
2 & 2 & $400 \times 600$ & 1782 & 5.54675 & 1.70 \\
2 & 4 & $400 \times 600$ & 1782 & 3.37292 & 2.79 \\
2 & 8 & $400 \times 600$ & 1782 & 2.53256 & 3.71 \\

4 & 1 & $800 \times 1200$ & 3596 & 40.213 & 1.00 \\
4 & 2 & $800 \times 1200$ & 3596 & 23.1807 & 1.73 \\
4 & 4 & $800 \times 1200$ & 3596 & 15.0471 & 2.67 \\
4 & 8 & $800 \times 1200$ & 3596 & 10.8915 & 3.69 \\
\bottomrule
\end{tabular}
\end{table}

\subsection{MPI+CUDA программа}

\begin{table}[H]
\centering
\caption{Результаты MPI+CUDA расчетов (сетка $2400 \times 3600$)}
\begin{tabular}{cccccc}
\toprule
\makecell{Количество \\ процессов MPI} & \makecell{Число точек \\ сетки} & \makecell{Число \\ итераций} & \makecell{Общее время \\(с)} & \makecell{Время CG \\(с)} & Ускорение \\
\midrule
1 & $2400 \times 3600$ & 10242 & 66.083 & 42.489 & 44.5 \\
2 & $2400 \times 3600$ & 10242 & 48.342 & 28.551 & 60.8 \\
\bottomrule
\end{tabular}
\end{table}

\begin{table}[H]
\centering
\caption{Детальное время выполнения операций MPI+CUDA (сетка $2400 \times 3600$)}
\begin{tabular}{lcc}
\toprule
Операция & 1 процесс (с) & 2 процесса (с) \\
\midrule
\textbf{Инициализация и завершение} & & \\
Инициализация (Init) & 6.927 & 3.458 \\
Настройка (Setup) & 6.543 & 3.264 \\
Сбор результатов (Gather) & 0.103 & 0.080 \\
\midrule
\textbf{Метод сопряженных градиентов (CG)} & 42.489 & 28.551 \\
\midrule
\textbf{Копирование данных CPU $\leftrightarrow$ GPU} & & \\
Копирование на GPU & 0.181 & 0.291 \\
Копирование с GPU & 0.021 & 0.310 \\
\textit{Всего копирование} & \textit{0.202} & \textit{0.601} \\
\midrule
\textbf{Вычисления на GPU} & & \\
Всего вычислений на GPU & 42.162 & 27.702 \\
Применение оператора A & 5.862 & 3.162 \\
Решение с предобуславливателем D & 5.404 & 2.964 \\
Скалярные произведения & 20.664 & 16.889 \\
Обновление векторов & 6.703 & 3.752 \\
Обновление направления p & 3.524 & 1.979 \\
\midrule
\textbf{Коммуникации MPI} & 0.075 & 1.267 \\
\midrule
\textbf{Общее время} & 66.083 & 48.342 \\
\bottomrule
\end{tabular}
\end{table}

\begin{table}[H]
\centering
\caption{Сравнение всех реализаций для сетки $2400 \times 3600$}
\begin{tabular}{cccc}
\toprule
Реализация & Конфигурация & Время (с) & Ускорение \\
\midrule
Последовательная & 1 процесс & 2939.49 & 1.00 \\
OpenMP & 20 потоков & 431.988 & 6.81 \\
MPI+OpenMP & 20 процессов, 8 потоков & 255.755 & 11.49 \\
MPI+CUDA & 1 процесс & 66.083 & 44.5 \\
MPI+CUDA & 2 процесса & 48.342 & 60.8 \\
\bottomrule
\end{tabular}
\end{table}

\newpage
\subsection{Визуализация решения}
\begin{figure}[h]
    \centering
    \includegraphics[width=0.7\linewidth]{solution_800_1200_8_omp_3d.jpg}
\end{figure}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.65\linewidth]{solution_800_1200_8_omp_contour.jpg}
\end{figure}


\newpage
\section{Анализ результатов}

\subsection{Сходимость метода}

\begin{itemize}
\item Метод сопряженных градиентов демонстрирует устойчивую сходимость на всех сетках
\item Число итераций растет с увеличением размера сетки
\end{itemize}

\subsection{Эффективность параллелизации OpenMP для больших сеток}

\begin{itemize}
\item \textbf{Для сетки 400×600} наблюдается хорошее ускорение:
\begin{itemize}
\item 4 потока: ускорение 1.67
\item 8 потоков: ускорение 2.56  
\item 16 потоков: ускорение 3.11
\end{itemize}

\item \textbf{Для сетки 800×1200} ускорение не столь большое:
\begin{itemize}
\item 8 потоков: ускорение 1.52
\item 16 потоков: ускорение 1.88
\item 32 потока: ускорение 1.59
\end{itemize}
\end{itemize}

\subsection{Эффективность параллелизации MPI для больших сеток}

\begin{itemize}
\item \textbf{Для сетки 400×600}:
\begin{itemize}
\item 4 процесса: ускорение 1.85
\item 8 процессов: ускорение 3.51
\item 16 процессов: ускорение 4.97
\end{itemize}

\item \textbf{Для сетки 800×1200}:
\begin{itemize}
\item 8 процессов: ускорение 1.91
\item 16 процессов: ускорение 2.97
\item 20 процессов: ускорение 3.23
\end{itemize}
\end{itemize}

\newpage
\subsection{Анализ масштабируемости}

\begin{figure}[H]
\centering
\begin{tikzpicture}
\begin{axis}[
    title={Ускорение OpenMP реализации},
    xlabel={Число потоков},
    ylabel={Ускорение},
    xmin=0, xmax=35,
    ymin=0, ymax=4,
    xtick={2,4,8,16,32},
    legend pos=north west,
    grid=major,
    width=0.8\textwidth,
    height=0.5\textwidth
]

\addplot[color=blue, mark=*] coordinates {(2,1) (4,1.67) (8,2.56) (16,3.11)};
\addplot[color=red, mark=square] coordinates {(4,1) (8,1.52) (16,1.88) (32,1.59)};
\legend{Сетка 400×600, Сетка 800×1200}
\end{axis}
\end{tikzpicture}
\caption{Сравнение ускорения OpenMP для разных размеров сеток}
\end{figure}

\begin{figure}[H]
\centering
\begin{tikzpicture}
\begin{axis}[
    title={Ускорение MPI реализации},
    xlabel={Число процессов},
    ylabel={Ускорение},
    xmin=0, xmax=25,
    ymin=0, ymax=8,
    xtick={2,4,8,16,20},
    legend pos=north west,
    grid=major,
    width=0.8\textwidth,
    height=0.5\textwidth
]

\addplot[color=blue, mark=*] coordinates {(2,1) (4,1.85) (8,3.51) (16,4.97)};
\addplot[color=red, mark=square] coordinates {(4,1) (8,1.91) (16,2.97) (20,3.23)};
\legend{Сетка 400×600, Сетка 800×1200}
\end{axis}
\end{tikzpicture}
\caption{Сравнение ускорения MPI для разных размеров сеток}
\end{figure}

\begin{figure}[H]
\centering
\begin{tikzpicture}
\begin{axis}[
    title={Ускорение гибридной MPI+OpenMP реализации},
    xlabel={Число потоков OpenMP на процесс},
    ylabel={Ускорение},
    xmin=0, xmax=9,
    ymin=0, ymax=4,
    xtick={1,2,4,8},
    legend pos=north west,
    grid=major,
    width=0.8\textwidth,
    height=0.5\textwidth
]

\addplot[color=blue, mark=*] coordinates {(1,1.00) (2,1.70) (4,2.79) (8,3.71)};
\addplot[color=red, mark=square] coordinates {(1,1.00) (2,1.73) (4,2.67) (8,3.69)};
\legend{Сетка 400×600 (2 процесса), Сетка 800×1200 (4 процесса)}
\end{axis}
\end{tikzpicture}
\caption{Сравнение ускорения гибридной MPI+OpenMP для разных размеров сеток}
\end{figure}

\begin{enumerate}
\item \textbf{Для малых сеток (40×40)}: 
\begin{itemize}
\item \textbf{OpenMP}: Накладные расходы превышают выигрыш от параллелизма (ускорение 0.76 на 1 потоке относительно последовательной программы, 1.07 на 4 потоках)
\item \textbf{MPI}: Наблюдается ускорение относительно последовательной программы: 1.07 на 1 процессе, 1.67 на 2 процессах, 1.50 на 4 процессах
\item \textbf{Гибридная MPI+OpenMP}: Для малых сеток гибридный подход показывает результаты, близкие к чистой MPI реализации (0.97 на 1 процессе с 4 потоками, 1.19 на 2 процессах с 4 потоками), что объясняется преобладанием накладных расходов над выигрышем от параллелизма
\end{itemize}

\item \textbf{Для средних сеток (400×600)}: 
\begin{itemize}
\item \textbf{OpenMP}: Наблюдается хорошее ускорение до 16 потоков (ускорение 3.11) благодаря достаточному объему вычислений на поток, хорошему балансу между вычислениями и накладными расходами, эффективному использованию кэша
\item \textbf{MPI}: Отличная масштабируемость до 16 процессов (ускорение 4.97 относительно 2 процессов), что значительно лучше OpenMP (ускорение 3.11)
\item \textbf{Гибридная MPI+OpenMP}: Демонстрирует эффективное масштабирование по потокам внутри процессов: ускорение от 1.00 (1 поток на процесс) до 3.71 (8 потоков на процесс) при использовании 2 процессов.
\end{itemize}

\item \textbf{Для больших сеток (800×1200)}: 
\begin{itemize}
\item \textbf{OpenMP}: Ускорение ограничено из-за того, что на сервере до 20 ядер в сумме и 32 потока не выполняются независимо, из-за этого возникают накладные расходы, которые замедляют программу
\item \textbf{MPI}: Показывает хорошую масштабируемость до 20 процессов (ускорение 3.23 относительно 4 процессов), что более чем в 2 раза превосходит OpenMP.
\item \textbf{Гибридная MPI+OpenMP}:   Демонстрирует практически идентичное ускорение для сеток 400×600 и 800×1200 при одинаковом числе потоков на процесс, что подтверждает его эффективность для больших задач.
\end{itemize}

\newpage
\item \textbf{Для очень больших сеток (2400×3600)}: 
\begin{itemize}
\item \textbf{Последовательная программа}: Время выполнения составляет 2939.49 секунд, что демонстрирует необходимость параллелизации для таких задач
\item \textbf{OpenMP (20 потоков)}: Ускорение 6.81, что показывает хорошую эффективность для задач с большим объемом вычислений
\item \textbf{MPI+OpenMP (20 процессов, 8 потоков)}: Наилучший результат среди CPU реализаций - ускорение 11.49, демонстрируя эффективность гибридного подхода для очень больших задач
\item \textbf{MPI+CUDA}: Ускорение 44.5 - наилучший результат среди всех реализаций.
\end{itemize}

\vspace{0.5cm}
\textbf{Анализ производительности MPI+CUDA:}
\begin{itemize}
\item \textbf{Вычисления на GPU}: Выполняются очень эффективно - 42.2 с для 1 процесса и 27.7 с для 2 процессов.
\item \textbf{Коммуникации MPI}: Для 1 процесса коммуникации практически отсутствуют (0.075 с), так как нет обмена данными между процессами. Для 2 процессов время коммуникаций составляет 1.267 секунды, что включает обмен граничными данными между соседними процессами. Коммуникации выполняются синхронно через \texttt{MPI\_Sendrecv}.
\end{itemize}


\end{enumerate}

\section{Исходный код и репозиторий}

\hspace{0.5cm}
Весь исходный код проекта, включая последовательную и параллельные реализации, а также данный отчет, размещены в Git-репозитории:

\begin{center}
\texttt{https://github.com/Pikudan/SKModel.git}
\end{center}

\vspace{0.5cm}
Структура репозитория:
\begin{itemize}
\item \texttt{main\_seq.cpp} - последовательная реализация (ветка \texttt{sequential})
\item \texttt{main\_openmp.cpp} - реализация с использованием OpenMP (ветка \texttt{openmp})
\item \texttt{main\_mpi.cpp} - реализация с использованием MPI (ветка \texttt{mpi})
\item \texttt{main\_hybrid.cpp} - гибридная MPI+OpenMP реализация (ветка \texttt{hybrid})
\item \texttt{main\_mpi\_cuda.cpp} - реализация MPI+CUDA (ветка \texttt{mpi-cuda})
\item \texttt{Makefile} - файл сборки для MPI+CUDA версии
\item \texttt{*.lsf} - скрипты для запуска на кластере IBM Polus
\item \texttt{main.pdf} - отчет
\end{itemize}


\vspace{0.5cm}
Каждая параллельная реализация находится в отдельной ветке Git, что позволяет легко переключаться между версиями и сравнивать результаты. Финальная версия со всеми реализациями находится в ветке \texttt{main}.

\newpage
\section{Выводы}

\begin{enumerate}

\item Разработаны параллельные реализации с использованием OpenMP и MPI для решения задачи Дирихле для уравнения Пуассона в криволинейной области:
\begin{itemize}
\item OpenMP реализация демонстрирует хорошую эффективность для средних сеток (ускорение до 3.11 на 16 потоках)
\item MPI реализация показывает отличную масштабируемость для больших сеток, превосходя OpenMP (ускорение 4.97 на 16 процессах относительно 2 процессов против 3.11 на 16 потоках OpenMP для сетки 400×600)
\item Гибридная MPI+OpenMP реализация демонстрирует отличнуюэффективность, для сетки $400 \times 600$ с 2 процессами и сетки $800 \times 1200$ с 4 процессами ускорение совпадает, что показывает эффективность гибридного подхода для больших задач.
\end{itemize}
\item Наблюдается \textbf{закон Амдала}: с ростом числа потоков/процессов эффективность параллелизации снижается, но для MPI этот эффект выражен слабее.
\item Число итераций и численное решение остается постоянным для фиксированной сетки при разном числе потоков/процессов, что подтверждает детерминированность алгоритма
\item \textbf{MPI+CUDA реализация} демонстрирует наилучшую производительность среди всех реализаций для очень больших сеток (2400×3600)

\end{enumerate}
\end{document}